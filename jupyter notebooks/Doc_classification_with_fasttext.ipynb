{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1597676c",
   "metadata": {},
   "source": [
    "# Trying NLP lightweight models to classify documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473df1f5",
   "metadata": {},
   "source": [
    "# Neccesary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c58464",
   "metadata": {
    "id": "wtUst8J2ikFj"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "# import torch.utils.data as data\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import os \n",
    "import pandas as pd\n",
    "import pytesseract\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3eaa47",
   "metadata": {},
   "source": [
    "# Install the library with the help of which we will get text from the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f44e020",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "zxs6EE5Xnkwl",
    "outputId": "6c5d5915-28c5-4b4e-9ead-5828710698a7"
   },
   "outputs": [],
   "source": [
    "# Note its a tricky library and in each OS needs to installed uniquely \n",
    "# ! pip install pytesseract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f1c776",
   "metadata": {},
   "source": [
    "# Extract texts from all the documents and save all txt files in the appropriate folder. Note: Grab a coffee, go breath some fresh air as it might take long "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fd6aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# data_dir = '/home/ml_user/Downloads/final_project/train/'\n",
    "\n",
    "# destination_dir = '/home/ml_user/Downloads/final_project/train_texts/'\n",
    "# if not os.path.exists(destination_dir):\n",
    "#     os.mkdir(destination_dir)\n",
    "\n",
    "# for folder in os.listdir(data_dir):\n",
    "\n",
    "#     current_directory = os.path.join(destination_dir, folder)\n",
    "\n",
    "#     if not os.path.exists(current_directory):\n",
    "#         os.mkdir(current_directory)\n",
    "#     for image_name in tqdm(os.listdir(os.path.join(data_dir, folder))):\n",
    "\n",
    "#         image = Image.open(os.path.join(data_dir + folder, image_name)).convert(\"RGB\")\n",
    "\n",
    "#         ocr_df = pytesseract.image_to_data(image, output_type='data.frame')\n",
    "#         ocr_df = ocr_df.dropna().reset_index(drop=True)\n",
    "#         float_cols = ocr_df.select_dtypes('float').columns\n",
    "#         ocr_df[float_cols] = ocr_df[float_cols].round(0).astype(int)\n",
    "#         ocr_df = ocr_df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "#         try:\n",
    "#             words = ' '.join([word for word in ocr_df.text if str(word) != 'nan'])\n",
    "#         except:\n",
    "#             print('Could not find text for this image')\n",
    "#             print(image_name, folder)\n",
    "#             words = ''\n",
    "\n",
    "#         with open(os.path.join(destination_dir + folder, image_name[:-3] + 'txt'), \"w\") as dest_file:\n",
    "#             dest_file.write(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22585f0",
   "metadata": {},
   "source": [
    "# Extract the text from images and get the dataframe with text and label columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214bfa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_dir = '/home/ml_user/Downloads/final_project/train/'\n",
    "destination_dir = '/home/ml_user/Downloads/final_project/train_texts/'\n",
    "\n",
    "if not os.path.exists(destination_dir):\n",
    "    os.mkdir(destination_dir)\n",
    "\n",
    "data = pd.DataFrame(columns=['text', 'label'])\n",
    "\n",
    "for folder in os.listdir(data_dir):\n",
    "    count = 0 \n",
    "    current_directory = os.path.join(destination_dir, folder)\n",
    "\n",
    "    if not os.path.exists(current_directory):\n",
    "        os.mkdir(current_directory)\n",
    "    for image_name in tqdm(os.listdir(os.path.join(data_dir, folder))):\n",
    "        if count < 10:\n",
    "            image = Image.open(os.path.join(data_dir + folder, image_name)).convert(\"RGB\")\n",
    "\n",
    "            ocr_df = pytesseract.image_to_data(image, output_type='data.frame')\n",
    "            ocr_df = ocr_df.dropna().reset_index(drop=True)\n",
    "            float_cols = ocr_df.select_dtypes('float').columns\n",
    "            ocr_df[float_cols] = ocr_df[float_cols].round(0).astype(int)\n",
    "            ocr_df = ocr_df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "            words = ' '.join([word for word in ocr_df.text if str(word) != 'nan'])\n",
    "\n",
    "            data = data.append({'text': words, 'label': folder}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe49f505",
   "metadata": {},
   "source": [
    "# Read the texts from the saved folder of txt files and create the dataframe with text and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b128fb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = '/home/ml_user/Downloads/final_project/train_texts/'\n",
    "\n",
    "# text_list = []\n",
    "# labels_list = []\n",
    "# data = pd.DataFrame(columns=['text', 'label'])\n",
    "\n",
    "# for folder in os.listdir(data_dir):\n",
    "#     current_directory = os.path.join(data_dir, folder)\n",
    "#     for text_of_image in tqdm(os.listdir(current_directory)):\n",
    "        \n",
    "#             text_i = open(os.path.join(data_dir + folder, text_of_image),encoding = 'latin-1').read()\n",
    "#             text_i = text_i.replace('\\n',' ')\n",
    "#             text_i = text_i.replace(',',' ')\n",
    "            \n",
    "#             if len(text_i) != 0: \n",
    "#                 data =data.append({'text': text_i, 'label': folder}, ignore_index=True)\n",
    "#             else:\n",
    "#                 data =data.append({'text': text_i, 'label': folder}, ignore_index=True)\n",
    "\n",
    "# print(f'The shape of the data is {data.shape}')\n",
    "\n",
    "# print(data['label'].value_counts())\n",
    "\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3393f5c4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_y-ll3Wvt1wk",
    "outputId": "1a13d80e-a1f1-449b-a1c2-5c4ce6c77c53"
   },
   "source": [
    "# Map the classes to corresponding labels from 0 - 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041fb42e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GFW7rJl5mzpc",
    "outputId": "23ad2df5-262e-453c-e633-036d368c09f1"
   },
   "outputs": [],
   "source": [
    "def get_label_mapping(value):\n",
    "    return class_mapping[value]\n",
    "\n",
    "class_mapping = {\n",
    "    'memorandum': 0,\n",
    "    'email': 1, \n",
    "    'cv': 2, \n",
    "    'report': 3, \n",
    "    'newspaper': 4 ,\n",
    "    'survey': 5,\n",
    "    'specification':6,\n",
    "    'publication':7,\n",
    "    'invoice':8,\n",
    "    'letter':9, \n",
    "    'ad':10, \n",
    "    'handwritten':11,\n",
    "    'file':12,\n",
    "}\n",
    "\n",
    "data['label'] = data['label'].apply(get_label_mapping)\n",
    "\n",
    "print(data['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125a5ef2",
   "metadata": {},
   "source": [
    "# Understand what portion of your dataset contains more than 100 symbols of text extracted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328e55a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (data['text'].str.len() > 100)\n",
    "\n",
    "print(data.loc[mask].shape[0] / data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebab223f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zs-V1PvPqY7j",
    "outputId": "c3abd4a6-5495-43e1-f2d4-d62cbc207e0a"
   },
   "outputs": [],
   "source": [
    "print(data.isna().sum())\n",
    "data = data.dropna()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f719fd0f",
   "metadata": {},
   "source": [
    "# Text preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c46417",
   "metadata": {},
   "source": [
    "# Neccessary installations and imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d8077d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nltk\n",
    "# pip install -U pip setuptools wheel\n",
    "# pip install -U spacy\n",
    "# python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cded2775",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from html import unescape\n",
    "import os\n",
    "import spacy\n",
    "import nltk \n",
    "from nltk.stem import SnowballStemmer\n",
    "nltk.download('words')\n",
    "\n",
    "\n",
    "try:\n",
    "    spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "except:\n",
    "    os.system('python -m spacy download en_core_web_sm')\n",
    "    spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "snowball = SnowballStemmer(\"english\")\n",
    "stops_spacy = sorted(spacy.lang.en.stop_words.STOP_WORDS)\n",
    "stops_spacy.extend([\"is\", \"to\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f92195",
   "metadata": {},
   "source": [
    "# Define all auxiliary functions for text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d2895b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textLower(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \n",
    "    text = ''.join([char if char.isalnum() or char == ' ' else ' ' for char in text])\n",
    "    text = ' '.join(text.split())  # remove multiple whitespace\n",
    "    \n",
    "    return text\n",
    "\n",
    "def normalize(text):\n",
    "    \n",
    "    # replace urls\n",
    "    soup = BeautifulSoup(unescape(text), 'html')\n",
    "    for a_tag in soup.find_all('a'):\n",
    "        a_tag.string = 'URL'\n",
    "    \n",
    "    text = soup.text\n",
    "    return text\n",
    "\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "def remove_stopwords_spacy(text, stopwords=stops_spacy):\n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords])\n",
    "    return text\n",
    "\n",
    "def lemmatize_spacy(text):\n",
    "    text = spacy_en(text)\n",
    "    lemmas = [token.lemma_ for token in text]\n",
    "    return \" \".join(lemmas)\n",
    "\n",
    "def remove_non_eng_words(text):\n",
    "\n",
    "    return \" \".join(w for w in nltk.wordpunct_tokenize(text) \\\n",
    "             if w.lower() in words or not w.isalpha())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb8c055",
   "metadata": {},
   "source": [
    "# Lets apply all the text preprocessings to text column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89abe179",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "n5At4sbWrmMB",
    "outputId": "ac078c5c-0735-4c46-e193-10322a9e1e09"
   },
   "outputs": [],
   "source": [
    "data[\"text\"] = data[\"text\"].apply(normalize)\n",
    "print('Stemming done')\n",
    "\n",
    "data[\"text\"] = data[\"text\"].apply(remove_punctuation)\n",
    "print('Stemming done')\n",
    "\n",
    "data[\"text\"] = data[\"text\"].apply(textLower)\n",
    "print('Stemming done')\n",
    "\n",
    "data[\"text\"] = data[\"text\"].apply(snowball.stem)\n",
    "print('Stemming done')\n",
    "\n",
    "# data[\"text\"] = data[\"text\"].apply(lemmatize_spacy)\n",
    "# print('Lemmatiztion done')\n",
    "\n",
    "data[\"text\"] = data[\"text\"].apply(remove_stopwords_spacy)\n",
    "print('Stopwords removal done')\n",
    "\n",
    "data[\"text\"] = data[\"text\"].apply(remove_non_eng_words)\n",
    "print('Non english words removal done')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9b71f9",
   "metadata": {},
   "source": [
    "# Lets print the wordcloud of words and get all the common words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbf987b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GM1mQe9wr2B7",
    "outputId": "e5877e8f-91df-4ab0-ffd2-b741f9e58bd8"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def word_counts(text, top_k=15, stopwords=None, only_alpha=False, min_len = 3):\n",
    "    words = [word for word in text.split(' ') if (word != '') and (len(word)>=min_len)]\n",
    "    if stopwords is not None:\n",
    "        stopwords = {stopword.lower() for stopword in stopwords}\n",
    "        words = [word for word in words if (word not in stopwords) and (len(word)>=min_len)]\n",
    "    if only_alpha:\n",
    "        words = [word for word in words if (word.isalpha()) and (len(word)>=min_len)]\n",
    "    counts = Counter(words)\n",
    "    return counts.most_common(top_k)\n",
    "\n",
    "word_counts(' '.join(data['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7162c6",
   "metadata": {},
   "source": [
    "# Training part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bde023",
   "metadata": {},
   "source": [
    "## Necessary installations and imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c485079c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8b3f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from fasttext import train_supervised"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e93c735",
   "metadata": {},
   "source": [
    "# Train test split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8210f157",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QjKSH3crr8dh",
    "outputId": "6bfc1812-653c-414e-a8e1-27f3ed00c49b"
   },
   "outputs": [],
   "source": [
    "X = data['text']\n",
    "y = data['label']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=124)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_val = np.array(X_val)\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067ccc59",
   "metadata": {},
   "source": [
    "# Training Naive Bayes and Multinomial Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ba2e48",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OigwH4eGsFla",
    "outputId": "201db4e3-639c-4d79-d850-354de0372e25"
   },
   "outputs": [],
   "source": [
    "train_scores, val_scores, models, model_names = [], [], [], []\n",
    "\n",
    "# ### count vectorizer + naive bayes\n",
    "\n",
    "nb = Pipeline([('countVec', CountVectorizer(lowercase=False, token_pattern='\\w+', min_df=3)),\n",
    "               ('clf', MultinomialNB()),])\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "y_pred = nb.predict(X_val)\n",
    "train_score = round(accuracy_score(nb.predict(X_train), y_train), 3)\n",
    "val_score = round(accuracy_score(y_pred, y_val), 3)\n",
    "\n",
    "print(f'train accuracy {train_score}')\n",
    "print(f'val accuracy {val_score}')\n",
    "\n",
    "train_scores.append(train_score)\n",
    "val_scores.append(val_score)\n",
    "models.append(nb)\n",
    "model_names.append('cv_ng1_nb')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c993b0d5",
   "metadata": {},
   "source": [
    "# Count vectorizer with bigrams + naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e4f271",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JoLC9NC9sLvl",
    "outputId": "4dac129c-8118-44fc-f47b-4fc87ea1f12b"
   },
   "outputs": [],
   "source": [
    "nb = Pipeline([('countVec', CountVectorizer(lowercase=False, token_pattern='\\w+', ngram_range=(1, 2), min_df=3)),\n",
    "               ('clf', MultinomialNB()),])\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "y_pred = nb.predict(X_val)\n",
    "train_score = round(accuracy_score(nb.predict(X_train), y_train), 3)\n",
    "val_score = round(accuracy_score(y_pred, y_val), 3)\n",
    "\n",
    "print(f'train accuracy {train_score}')\n",
    "print(f'val accuracy {val_score}')\n",
    "\n",
    "train_scores.append(train_score)\n",
    "val_scores.append(val_score)\n",
    "models.append(nb)\n",
    "model_names.append('cv_ng2_nb')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5d948f",
   "metadata": {},
   "source": [
    "# Count vectorizer with trigrams + naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb05c13",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j0xcxCBVsQfZ",
    "outputId": "3b7c3495-9871-4947-ddbb-66a9b85d0b68"
   },
   "outputs": [],
   "source": [
    "nb = Pipeline([('countVec', CountVectorizer(lowercase=False, token_pattern='\\w+', ngram_range=(1, 3), min_df=3)),\n",
    "               ('clf', MultinomialNB()),])\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "y_pred = nb.predict(X_val)\n",
    "train_score = round(accuracy_score(nb.predict(X_train), y_train), 3)\n",
    "val_score = round(accuracy_score(y_pred, y_val), 3)\n",
    "\n",
    "print(f'train accuracy {train_score}')\n",
    "print(f'val accuracy {val_score}')\n",
    "\n",
    "train_scores.append(train_score)\n",
    "val_scores.append(val_score)\n",
    "models.append(nb)\n",
    "model_names.append('cv_ng3_nb')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a72409f",
   "metadata": {},
   "source": [
    "# Tf-idf vectorizer with bigrams + naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83ee35c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c87IdnWjsZTM",
    "outputId": "2f263247-f25b-4100-8531-d74a1dd9fef0"
   },
   "outputs": [],
   "source": [
    "nb = Pipeline([('tfidf', TfidfVectorizer(lowercase=False, token_pattern='\\w+', ngram_range=(1, 2), \n",
    "                                         min_df=3)),\n",
    "               ('clf', MultinomialNB()),])\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "y_pred = nb.predict(X_val)\n",
    "train_score = round(accuracy_score(nb.predict(X_train), y_train), 3)\n",
    "val_score = round(accuracy_score(y_pred, y_val), 3)\n",
    "\n",
    "print(f'train accuracy {train_score}')\n",
    "print(f'val accuracy {val_score}')\n",
    "\n",
    "train_scores.append(train_score)\n",
    "val_scores.append(val_score)\n",
    "models.append(nb)\n",
    "model_names.append('tf_ng2_nb')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20260924",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ce1195",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LUZi66gRse4Y",
    "outputId": "3b739a42-168f-4939-b3a8-9fadc899701c"
   },
   "outputs": [],
   "source": [
    "logreg = Pipeline([('countVec', CountVectorizer(lowercase=False, token_pattern='\\w+', max_features=100)),\n",
    "                   ('clf', LogisticRegression(random_state=42, solver='liblinear')),])\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_val)\n",
    "train_score = round(accuracy_score(y_train, logreg.predict(X_train)), 3)\n",
    "val_score = round(accuracy_score(y_val, y_pred), 3)\n",
    "\n",
    "print(f'train accuracy {train_score}')\n",
    "print(f'val accuracy {val_score}')\n",
    "\n",
    "train_scores.append(train_score)\n",
    "val_scores.append(val_score)\n",
    "models.append(logreg) \n",
    "model_names.append('cv_ng1_lr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f63dfbe",
   "metadata": {},
   "source": [
    "# Count vectorizer with bigrams + logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d065be67",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = Pipeline([('countVec', CountVectorizer(lowercase=False, token_pattern='\\w+', ngram_range=(1, 2), \n",
    "                                                min_df=3)),\n",
    "                   ('clf', LogisticRegression(random_state=42, solver='liblinear')),])\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_val)\n",
    "train_score = round(accuracy_score(y_train, logreg.predict(X_train)), 3)\n",
    "val_score = round(accuracy_score(y_val, y_pred), 3)\n",
    "\n",
    "print(f'train accuracy {train_score}')\n",
    "print(f'val accuracy {val_score}')\n",
    "\n",
    "train_scores.append(train_score)\n",
    "val_scores.append(val_score)\n",
    "models.append(logreg)\n",
    "model_names.append('cv_ng2_lr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b042665",
   "metadata": {},
   "source": [
    "# Count vectorizer with trigrams + logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4803442",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_v_9mlmhsqZP",
    "outputId": "f9dea9b8-72e6-4587-a08e-daf626757bf5"
   },
   "outputs": [],
   "source": [
    "logreg = Pipeline([('countVec', CountVectorizer(lowercase=False, token_pattern='\\w+', ngram_range=(1, 3), \n",
    "                                                min_df=3)),\n",
    "                   ('clf', LogisticRegression(random_state=42, solver='liblinear')),])\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_val)\n",
    "train_score = round(accuracy_score(y_train, logreg.predict(X_train)), 3)\n",
    "val_score = round(accuracy_score(y_val, y_pred), 3)\n",
    "\n",
    "print(f'train accuracy {train_score}')\n",
    "print(f'val accuracy {val_score}')\n",
    "\n",
    "train_scores.append(train_score)\n",
    "val_scores.append(val_score)\n",
    "models.append(logreg)\n",
    "model_names.append('cv_ng3_lr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77909af4",
   "metadata": {},
   "source": [
    "# Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cf2194",
   "metadata": {
    "id": "FF24k9vktLIp"
   },
   "outputs": [],
   "source": [
    "def to_fasttext_format(data: list, labels: list, save_path: str=None):\n",
    "    ft_data = []\n",
    "    for d, l in zip(data, labels):\n",
    "        ft_data.append(\"__label__{} {}\".format(l, d))\n",
    "    if save_path:\n",
    "        np.savetxt(save_path, ft_data, fmt='%s')\n",
    "    else:\n",
    "        return ft_data\n",
    "    \n",
    "def train_fasttext(X_train, y_train, wordNgrams=1, minCount=1, ft_train_path=\"./tmp_train.txt\", **kwargs):\n",
    "    \n",
    "    to_fasttext_format(X_train, y_train, save_path=ft_train_path)\n",
    "    ft_model = train_supervised(ft_train_path, wordNgrams=wordNgrams, minCount=minCount, epoch=10, loss=\"softmax\",  **kwargs)\n",
    "    train_preds = [i[0].split('_')[-1] for i in ft_model.predict(list(X_train))[0]]\n",
    "\n",
    "    train_score = round(accuracy_score(np.array(train_preds).astype(np.integer), y_train), 3)\n",
    "    print(f'train accuracy {train_score}')\n",
    "    \n",
    "    return ft_model, train_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9acc54",
   "metadata": {},
   "source": [
    "# Training fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa144f56",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CxkleoQ5tUEZ",
    "outputId": "d9f41eb6-dc47-4670-d767-6cb84a0675d1"
   },
   "outputs": [],
   "source": [
    "ft_model, train_score = train_fasttext(X_train, y_train)\n",
    "val_preds = [i[0].split('_')[-1] for i in ft_model.predict(list(X_val))[0]]\n",
    "\n",
    "val_score = round(accuracy_score(y_val, np.array(val_preds).astype(np.integer)), 3)\n",
    "\n",
    "print(f'val accuracy {val_score}')\n",
    "\n",
    "\n",
    "train_scores.append(train_score)\n",
    "val_scores.append(val_score)\n",
    "models.append(ft_model)\n",
    "model_names.append('ft_ng1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc623830",
   "metadata": {},
   "source": [
    "# Fasttext with trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7005aaf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VhHNyo23taiJ",
    "outputId": "59d33210-745e-4802-a5b1-b41c196c24f9"
   },
   "outputs": [],
   "source": [
    "ft_model, train_score = train_fasttext(X_train, y_train, wordNgrams=3)\n",
    "val_preds = [i[0].split('_')[-1] for i in ft_model.predict(list(X_val))[0]]\n",
    "val_score = round(accuracy_score(y_val, np.array(val_preds).astype(np.integer)), 3)\n",
    "\n",
    "print(f'val accuracy {val_score}')\n",
    "\n",
    "train_scores.append(train_score)\n",
    "val_scores.append(val_score)\n",
    "models.append(ft_model)\n",
    "model_names.append('ft_ng3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0318354",
   "metadata": {},
   "source": [
    "# Fasttext with charngrams 3,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514b3453",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model, train_score = train_fasttext(X_train, y_train, minn=3, maxn=3)\n",
    "val_preds = [i[0].split('_')[-1] for i in ft_model.predict(list(X_val))[0]]\n",
    "val_score = round(accuracy_score(y_val, np.array(val_preds).astype(np.integer)), 3)\n",
    "\n",
    "print(f'val accuracy {val_score}')\n",
    "\n",
    "train_scores.append(train_score)\n",
    "val_scores.append(val_score)\n",
    "models.append(ft_model)\n",
    "model_names.append('ft_charngrams_33')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3d3d71",
   "metadata": {},
   "source": [
    "# Fasttext with charngrams 3,4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac3d7b3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-hiEdiy8vPZ8",
    "outputId": "662911a4-fc96-40e8-e7b8-9166cba5d581"
   },
   "outputs": [],
   "source": [
    "ft_model, train_score = train_fasttext(X_train, y_train, minn=3, maxn=4)\n",
    "val_preds = [i[0].split('_')[-1] for i in ft_model.predict(list(X_val))[0]]\n",
    "val_score = round(accuracy_score(y_val, np.array(val_preds).astype(np.integer)), 3)\n",
    "\n",
    "print(f'val accuracy {val_score}')\n",
    "\n",
    "train_scores.append(train_score)\n",
    "val_scores.append(val_score)\n",
    "models.append(ft_model)\n",
    "model_names.append('ft_charngrams_34')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a25cd3",
   "metadata": {},
   "source": [
    "# Fasttext with charngrams 4,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84c9b8a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6bHcMDSDvRey",
    "outputId": "28dfada2-66bc-43e5-d2dc-2526db99338d"
   },
   "outputs": [],
   "source": [
    "ft_model, train_score = train_fasttext(X_train, y_train, minn=4, maxn=5)\n",
    "val_preds = [i[0].split('_')[-1] for i in ft_model.predict(list(X_val))[0]]\n",
    "val_score = round(accuracy_score(y_val, np.array(val_preds).astype(np.integer)), 3)\n",
    "\n",
    "print(f'val accuracy {val_score}')\n",
    "\n",
    "train_scores.append(train_score)\n",
    "val_scores.append(val_score)\n",
    "models.append(ft_model)\n",
    "model_names.append('ft_charngrams_45')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8c0a34",
   "metadata": {},
   "source": [
    "# Fasttext with charngrams 5,7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd78453c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tGe-k57rvVGf",
    "outputId": "04a45bcc-7eec-42ea-dc9f-c52267d36ad0"
   },
   "outputs": [],
   "source": [
    "ft_model, train_score = train_fasttext(X_train, y_train, minn=5, maxn=7)\n",
    "val_preds = [i[0].split('_')[-1] for i in ft_model.predict(list(X_val))[0]]\n",
    "val_score = round(accuracy_score(y_val, np.array(val_preds).astype(np.integer)), 3)\n",
    "\n",
    "print(f'val accuracy {val_score}')\n",
    "\n",
    "train_scores.append(train_score)\n",
    "val_scores.append(val_score)\n",
    "models.append(ft_model)\n",
    "model_names.append('ft_charngrams_57')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6027044",
   "metadata": {},
   "source": [
    "# Plot all training and validation scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d84d07",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "A5lAeJuWvW1s",
    "outputId": "c625fe3d-a2af-4eb4-b187-5672642098bb"
   },
   "outputs": [],
   "source": [
    "plt.plot(train_scores)\n",
    "plt.plot(val_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
