{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72b4cbc7",
   "metadata": {},
   "source": [
    "# Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4e30c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "from torchvision.utils import make_grid\n",
    "import torch.nn.functional as F\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f039c5",
   "metadata": {},
   "source": [
    "# Copy sample data from one folder to another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a99983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# import shutil\n",
    "\n",
    "# data_dir = '/home/ml_user/Downloads/final_project/train/'\n",
    "# destination_dir = \"/home/ml_user/Downloads/Arman_final_project_sample/\"\n",
    "# parent_list = os.listdir(data_dir)\n",
    "# print(parent_list)\n",
    "\n",
    "# for folder in os.listdir(data_dir):\n",
    "#     count = 0 \n",
    "#     current_directory = os.path.join(destination_dir, folder)\n",
    "#     if not os.path.exists(current_directory):\n",
    "#         os.mkdir(current_directory)\n",
    "#     for image in tqdm(os.listdir(os.path.join(data_dir, folder))):\n",
    "#         if count < 1000:\n",
    "#             image_path = os.path.join(data_dir + folder, image)\n",
    "#             shutil.copy(image_path, os.path.join(destination_dir + folder, image))\n",
    "#         else:\n",
    "#             break\n",
    "#         count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e543f7a6",
   "metadata": {},
   "source": [
    "# How to mount google drive to colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12fd9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b869357c",
   "metadata": {},
   "source": [
    "# Check device whether cpu or gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9299064c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7c7e3e",
   "metadata": {},
   "source": [
    "# Data augmentation and normalization for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6723dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just normalization for validation\n",
    "\n",
    "# data_transforms = {\n",
    "#     'train': transforms.Compose([\n",
    "#         transforms.RandomResizedCrop(224),\n",
    "#         transforms.RandomHorizontalFlip(),\n",
    "#         transforms.ToTensor(),\n",
    "#         #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "#     ]),\n",
    "#     'val': transforms.Compose([\n",
    "#         transforms.Resize(256),\n",
    "#         transforms.CenterCrop(224),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "#     ]),\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8044ba",
   "metadata": {},
   "source": [
    "# How to read data with pytorch imagefolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275bdf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your data must be organized in separate folders each folder with the name the label and the images inside it \n",
    "# So if you have newspaper folder than all the newspaper images and files must be in that folder\n",
    "data_dir = '/home/ml_user/Downloads/Arman_final_project_sample/'\n",
    "\n",
    "#image_datasets = datasets.ImageFolder(data_dir, data_transforms['train'])\n",
    "image_datasets = datasets.ImageFolder(data_dir)\n",
    "image_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d526386",
   "metadata": {},
   "source": [
    "# How to calculate mean and standart deviation of the images and apply transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84eb42fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_and_std():\n",
    "    batch_size = 128\n",
    "    image_datasets.transform =  transforms.Compose([\n",
    "                    transforms.Resize((224, 224)),\n",
    "                    transforms.ToTensor()])\n",
    "\n",
    "    loader = torch.utils.data.DataLoader(image_datasets, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    psum    = torch.tensor([0.0, 0.0, 0.0])\n",
    "    psum_sq = torch.tensor([0.0, 0.0, 0.0])\n",
    "\n",
    "    for inputs, labels in loader:\n",
    "        psum    += inputs.sum(axis=[0, 2, 3])\n",
    "        psum_sq += (inputs ** 2).sum(axis=[0, 2, 3])\n",
    "\n",
    "    # pixel count\n",
    "    count = len(image_datasets) * 224 * 224\n",
    "\n",
    "    # mean and std\n",
    "    total_mean = psum / count\n",
    "    total_var  = (psum_sq / count) - (total_mean ** 2)\n",
    "    total_std  = torch.sqrt(total_var)\n",
    "\n",
    "    # output\n",
    "    print('mean: '  + str(total_mean))\n",
    "    print('std:  '  + str(total_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d7b687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_mean_and_std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26664085",
   "metadata": {},
   "source": [
    "# How to split split train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a418e05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as torch_data\n",
    "\n",
    "train_set_size = int(len(image_datasets) * 0.8)\n",
    "valid_set_size = len(image_datasets) - train_set_size\n",
    "train_set, valid_set = torch_data.random_split(image_datasets, [train_set_size, valid_set_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168f922d",
   "metadata": {},
   "source": [
    "# How to apply the transformations to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9d61a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_transforms = transforms.Compose([\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.RandomResizedCrop((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                # Here normalize uses the computed mean and std\n",
    "                transforms.Normalize(mean=0.9151, std=0.2046)\n",
    "            ])\n",
    "\n",
    "test_data_transforms = transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                # Here normalize uses the computed mean and std\n",
    "                transforms.Normalize(mean=0.9151, std=0.2046)\n",
    "            ])\n",
    "\n",
    "train_set.dataset.transform = train_data_transforms\n",
    "valid_set.dataset.transform = test_data_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbecac31",
   "metadata": {},
   "source": [
    "# Get the train and validation loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53798984",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True, num_workers=6)\n",
    "val_loader = torch.utils.data.DataLoader(valid_set, batch_size=32, shuffle=True, num_workers=6)\n",
    "\n",
    "#dataset_sizes = {x: len(image_datasets[x]) for x in ['train']}\n",
    "\n",
    "class_names = image_datasets.classes\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a68681",
   "metadata": {},
   "source": [
    "# Make sure the train and validation loader are set correctly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7514b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, target in train_loader:\n",
    "    print(x.shape)\n",
    "    print(target)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0510a62b",
   "metadata": {},
   "source": [
    "# Plot the images with their labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eb527b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(formatter=dict(int=lambda x: f'{x:5}')) # to widen the printed array\n",
    "\n",
    "# Grab the first batch of 10 images\n",
    "for images,labels in train_loader: \n",
    "    break\n",
    "\n",
    "# Print the labels\n",
    "print('Label:', labels.numpy())\n",
    "print('Class: ', *np.array([class_names[i] for i in labels]))\n",
    "\n",
    "# Print the images\n",
    "im = make_grid(images, nrow=5)  # the default nrow is 8\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.imshow(np.transpose(im.numpy(), (1, 2, 0)));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce3a033",
   "metadata": {},
   "source": [
    "# Try couple of architectures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2c9d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 3, 1)  # changed from (1, 6, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3, 1)\n",
    "        self.fc1 = nn.Linear(54*54*16, 120)   \n",
    "        self.fc2 = nn.Linear(120,84)\n",
    "        self.fc3 = nn.Linear(84, 13)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = F.relu(self.conv1(X))\n",
    "        X = F.max_pool2d(X, 2, 2)\n",
    "        X = F.relu(self.conv2(X))\n",
    "        X = F.max_pool2d(X, 2, 2)\n",
    "        X = X.view(-1, 54*54*16)\n",
    "        X = F.relu(self.fc1(X))\n",
    "        X = F.relu(self.fc2(X))\n",
    "        X = self.fc3(X)\n",
    "        return F.log_softmax(X, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4e2670",
   "metadata": {},
   "source": [
    "# Another conv architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f109b2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 3, 1)  # changed from (1, 6, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3, 1)\n",
    "        self.conv3 = nn.Conv2d(16, 32, 3, 1)\n",
    "        self.fc1 = nn.Linear(52*52*32, 1024)   # changed from (4*4*16) to fit 32x32 images with 3x3 filters\n",
    "        self.fc2 = nn.Linear(1024,512)\n",
    "        self.fc3 = nn.Linear(512, 13)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = F.relu(self.conv1(X))\n",
    "        X = F.max_pool2d(X, 2, 2)\n",
    "        X = F.relu(self.conv2(X))\n",
    "        X = F.max_pool2d(X, 2, 2)\n",
    "        X = F.relu(self.conv3(X))\n",
    "        X = X.view(-1, 114*114*32)\n",
    "        X = F.relu(self.fc1(X))\n",
    "        X = F.relu(self.fc2(X))\n",
    "        X = self.fc3(X)\n",
    "        return F.log_softmax(X, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e14e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(101)\n",
    "model = ConvolutionalNetwork()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98aba32d",
   "metadata": {},
   "source": [
    "# How to count parameters of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38151324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for item in params:\n",
    "        print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709e5e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24593fe",
   "metadata": {},
   "source": [
    "# How to use the vgg model and adjust the last layer according number of classess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb4c918",
   "metadata": {},
   "outputs": [],
   "source": [
    "from traitlets.traitlets import ForwardDeclaredInstance\n",
    "from torchvision import models\n",
    "\n",
    "vgg16 = models.vgg16(pretrained=False)\n",
    "\n",
    "num_classes = 13\n",
    "\n",
    "vgg16.classifier = nn.Linear(25088, num_classes)\n",
    "vgg16.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc573205",
   "metadata": {},
   "source": [
    "## Define loss function & optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f91ba42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(vgg16.parameters(), lr=0.0001, weight_decay=0.015)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada44775",
   "metadata": {},
   "source": [
    "## Train the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de526243",
   "metadata": {},
   "source": [
    "# One way of training our model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2a4333",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "epochs = 5\n",
    "running_loss_history = []\n",
    "running_corrects_history = []\n",
    "val_running_loss_history = []\n",
    "val_running_corrects_history = []\n",
    "\n",
    "for e in tqdm(range(epochs)):\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "    val_running_loss = 0.0\n",
    "    val_running_corrects = 0.0\n",
    "  \n",
    "    for batch_idx, (inputs, labels) in tqdm(enumerate(train_loader)):\n",
    "        \n",
    "        \n",
    "#         if batch_idx % 10 == 0:\n",
    "#             print(f\"Batch {batch_idx} from {len(train_loader)} of epoch {e + 1}\")\n",
    "            \n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        vgg16.train()\n",
    "        outputs = vgg16(inputs)                      # predict using the model class\n",
    "\n",
    "        loss = criterion(outputs, labels)           # calculate the loss\n",
    "\n",
    "        optimizer.zero_grad()                       # make the gradients zero so as to not accumulate\n",
    "        loss.backward()                             # backward is to calculate the gradients\n",
    "        optimizer.step()                            # update the weights using the step fucntion \n",
    "\n",
    "        _, preds = torch.max(outputs, 1)            \n",
    "        running_loss += loss.item()                 \n",
    "        running_corrects += torch.sum(preds == labels.data)  \n",
    "\n",
    "        del inputs\n",
    "        del labels\n",
    "\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_labels in val_loader:\n",
    "                \n",
    "                #val_inputs = val_inputs.view(val_inputs.shape[0], -1)   # flattten if we have neural net architecture\n",
    "                val_inputs = val_inputs.to(device)\n",
    "                val_labels = val_labels.to(device)\n",
    "                val_outputs = vgg16(val_inputs)                    # predict\n",
    "                val_loss = criterion(val_outputs, val_labels)           # calculate loss\n",
    "\n",
    "                _, val_preds = torch.max(val_outputs, 1)\n",
    "                val_running_loss += val_loss.item()                                # add up validation loss\n",
    "                val_running_corrects += torch.sum(val_preds == val_labels.data)    # calcualte the exact number of correct predictions\n",
    "\n",
    "            epoch_loss = running_loss/ len(train_set)             \n",
    "            epoch_acc = running_corrects.float()/ len(train_set)\n",
    "\n",
    "            running_loss_history.append(epoch_loss)                    \n",
    "            running_corrects_history.append(epoch_acc)                 \n",
    "\n",
    "            val_epoch_loss = val_running_loss/len(valid_set) \n",
    "            val_epoch_acc = val_running_corrects.float()/ len(valid_set)   \n",
    "            val_running_loss_history.append(val_epoch_loss)\n",
    "            val_running_corrects_history.append(val_epoch_acc)\n",
    "            \n",
    "            vgg16.eval()\n",
    "            # Save the weights in your desired path \n",
    "            # torch.save(vgg16.state_dict(), f\"/content/drive/MyDrive/VGG_weights/vgg_weights_{epoch}.pth\")\n",
    "            torch.save(vgg16.state_dict(), f\"/home/ml_user/Webfountaine_full_course/Final_project/Final_working_code/VGG_weights/vgg_weights_{e}.pth\")\n",
    "            \n",
    "            print('epoch :', (e+1))                                   \n",
    "            print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n",
    "            print('validation loss: {:.4f}, validation acc {:.4f} '.format(val_epoch_loss, val_epoch_acc.item()))\n",
    "\n",
    "\n",
    "plt.plot(running_loss_history, label='training loss')\n",
    "plt.plot(val_running_loss_history, label='validation loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af53a4c",
   "metadata": {},
   "source": [
    "# Another way of training our model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4876ed",
   "metadata": {},
   "source": [
    "# Defining auxiliary methods for our training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c2f27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, device):\n",
    "    with torch.no_grad():\n",
    "        correct_preds, n_preds = 0, 0\n",
    "        for i, (X_i, y_i) in tqdm(enumerate(test_loader), \"Evaluating...\", total=len(test_loader)):\n",
    "            X_i = X_i.to(device)\n",
    "            y_i = y_i.to(device)\n",
    "            out_i = model(X_i)\n",
    "            correct_preds += sum(torch.argmax(out_i, dim=1) == y_i)\n",
    "            n_preds += len(y_i)\n",
    "     \n",
    "    return int(correct_preds) / n_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8044f033",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_loss(model, loader, criterion, device):\n",
    "    \n",
    "    \"\"\"\n",
    "    Compute the average loss for validation after each epoch \n",
    "    \"\"\"\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (batch_data, batch_labels) in tqdm(enumerate(loader), \"Computing validation loss...\", total=len(loader)):\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "\n",
    "            prediction = model(batch_data)\n",
    "            loss = criterion(prediction, batch_labels)\n",
    "            total_loss += loss.cpu()\n",
    "     \n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070bbb31",
   "metadata": {},
   "source": [
    "# When we have Cuda we cam get the device name along with the total memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce2f34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('__CUDA Device Name:',torch.cuda.get_device_name(0))\n",
    "# print('__CUDA Device Total Memory [GB]:',torch.cuda.get_device_properties(0).total_memory/1e9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98b58df",
   "metadata": {},
   "source": [
    "# Defining the actual training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a842f730",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available else 'cpu'\n",
    "vgg16.to(device)\n",
    "\n",
    "train_losses_batch = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    vgg16.train()\n",
    "    for batch_idx, (batch_data, batch_labels) in tqdm(enumerate(train_loader), f\"Training epoch {epoch}\", total=len(train_loader)):\n",
    "\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "\n",
    "        prediction = vgg16(batch_data)\n",
    "\n",
    "        loss = criterion(prediction, batch_labels)\n",
    "        train_losses_batch.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        vgg16.zero_grad()\n",
    "\n",
    "    # Free up the GPU so we can run the evals on it.\n",
    "    del batch_data\n",
    "    del batch_labels\n",
    "\n",
    "    val_loss = compute_avg_loss(vgg16, val_loader, criterion, device)\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    vgg16.eval()\n",
    "    #torch.save(vgg16.state_dict(), f\"/content/drive/MyDrive/VGG_weights/vgg_weights_{epoch}.pth\")\n",
    "    torch.save(vgg16.state_dict(), f\"/home/ml_user/Webfountaine_full_course/Final_project/Final_working_code/VGG_weights/vgg_weights_{epoch}.pth\")\n",
    "    \n",
    "    train_losses.append(sum(train_losses_batch) / len(train_losses_batch))\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    train_accuracies.append( evaluate(vgg16, train_loader, device) )\n",
    "    val_accuracies.append( evaluate(vgg16, val_loader, device) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc540f0",
   "metadata": {},
   "source": [
    "# Plotting the batch losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070f3a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Batch loss\")\n",
    "plt.plot(train_losses_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fbf6bb",
   "metadata": {},
   "source": [
    "# Plotting the train and validation lossess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799c8cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='Train loss')\n",
    "plt.plot(val_losses, label='Validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2bbf6a",
   "metadata": {},
   "source": [
    "# Plotting the train and validation accuraccies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0711163",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_accuracies, label='Train accuracy')\n",
    "plt.plot(val_accuracies, label='Validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b18639",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_accuracies)\n",
    "print(val_accuracies)\n",
    "print(optimizer)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
